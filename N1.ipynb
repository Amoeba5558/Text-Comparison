{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda34f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T02:51:38.263408Z",
     "iopub.status.busy": "2024-10-25T02:51:38.262446Z",
     "iopub.status.idle": "2024-10-25T02:51:38.269929Z",
     "shell.execute_reply": "2024-10-25T02:51:38.26859Z",
     "shell.execute_reply.started": "2024-10-25T02:51:38.263362Z"
    },
    "papermill": {
     "duration": 0.005135,
     "end_time": "2024-11-30T00:01:03.397422",
     "exception": false,
     "start_time": "2024-11-30T00:01:03.392287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "- https://www.kaggle.com/code/richolson/ai-math-olympiad-qwen2-5-72b for showing how to submit\n",
    "- https://www.kaggle.com/code/abdullahmeda/load-72b-awq-model-using-vllm-on-l4-x4\n",
    "- https://www.kaggle.com/code/huikang/qwen2-5-math-1-5b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40757fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:01:03.407475Z",
     "iopub.status.busy": "2024-11-30T00:01:03.407247Z",
     "iopub.status.idle": "2024-11-30T00:01:33.268074Z",
     "shell.execute_reply": "2024-11-30T00:01:33.267344Z"
    },
    "papermill": {
     "duration": 29.868067,
     "end_time": "2024-11-30T00:01:33.270103",
     "exception": false,
     "start_time": "2024-11-30T00:01:03.402036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9698f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:01:33.281083Z",
     "iopub.status.busy": "2024-11-30T00:01:33.280354Z",
     "iopub.status.idle": "2024-11-30T00:01:34.191044Z",
     "shell.execute_reply": "2024-11-30T00:01:34.190366Z"
    },
    "papermill": {
     "duration": 0.91794,
     "end_time": "2024-11-30T00:01:34.192985",
     "exception": false,
     "start_time": "2024-11-30T00:01:33.275045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "cutoff_time = time.time() + (4 * 60 + 45) * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62b78ac",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-30T00:01:34.203673Z",
     "iopub.status.busy": "2024-11-30T00:01:34.203098Z",
     "iopub.status.idle": "2024-11-30T00:05:32.276208Z",
     "shell.execute_reply": "2024-11-30T00:05:32.275379Z"
    },
    "papermill": {
     "duration": 238.080549,
     "end_time": "2024-11-30T00:05:32.278286",
     "exception": false,
     "start_time": "2024-11-30T00:01:34.197737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 00:01:55,271\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 00:02:22 awq_marlin.py:97] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 00:02:22 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 11-30 00:02:22 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1', speculative_config=None, tokenizer='/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-30 00:02:23 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-30 00:02:23 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 11-30 00:02:25 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-30 00:02:25 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-30 00:02:25 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-30 00:02:26 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-30 00:02:26 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-30 00:02:26 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-30 00:02:26 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-30 00:02:26 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-30 00:02:26 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-30 00:02:26 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-30 00:02:26 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 11-30 00:02:26 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m WARNING 11-30 00:02:26 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-30 00:02:26 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-30 00:02:26 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-30 00:02:26 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7a9623f0e1d0>, local_subscribe_port=37129, remote_subscribe_port=None)\n",
      "INFO 11-30 00:02:26 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-30 00:02:26 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-30 00:02:26 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n",
      "INFO 11-30 00:02:26 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3e5d70f0de4264bc46180d2c9a5ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-30 00:04:29 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "INFO 11-30 00:04:29 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 11-30 00:04:29 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-30 00:04:29 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "INFO 11-30 00:04:53 distributed_gpu_executor.py:57] # GPU blocks: 13920, # CPU blocks: 4096\n",
      "INFO 11-30 00:04:53 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 6.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-30 00:04:56 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-30 00:04:56 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-30 00:04:56 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-30 00:04:56 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-30 00:04:56 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-30 00:04:56 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-30 00:04:56 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-30 00:04:56 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-30 00:05:31 model_runner.py:1523] Graph capturing finished in 35 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 11-30 00:05:31 model_runner.py:1523] Graph capturing finished in 35 secs.\n",
      "INFO 11-30 00:05:31 model_runner.py:1523] Graph capturing finished in 35 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-30 00:05:31 model_runner.py:1523] Graph capturing finished in 35 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#llm_model_pth = '/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1'\n",
    "#llm_model_pth = '/kaggle/input/qwq-32b-preview/transformers/default/1'\n",
    "llm_model_pth = '/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1'\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    #dtype=\"half\",                # The data type for the model weights and activations\n",
    "    #max_num_seqs=128,              # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=32768,#4096*10,          # Model context length\n",
    "    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,      # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.96, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fbb9c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.296094Z",
     "iopub.status.busy": "2024-11-30T00:05:32.295795Z",
     "iopub.status.idle": "2024-11-30T00:05:32.299390Z",
     "shell.execute_reply": "2024-11-30T00:05:32.298749Z"
    },
    "papermill": {
     "duration": 0.014089,
     "end_time": "2024-11-30T00:05:32.300842",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.286753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6145971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.317936Z",
     "iopub.status.busy": "2024-11-30T00:05:32.317675Z",
     "iopub.status.idle": "2024-11-30T00:05:32.328607Z",
     "shell.execute_reply": "2024-11-30T00:05:32.328044Z"
    },
    "papermill": {
     "duration": 0.021191,
     "end_time": "2024-11-30T00:05:32.330032",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.308841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "thoughts = [\n",
    "    #'''你是一个乐于助人、人畜无害的数学天才。你应该使用 python 脚本并一步一步地思考，并把最后答案放在\\\\boxed{}中。''',\n",
    "    'Please use chained reasoning to put the answer in \\\\boxed{}.',\n",
    "    'Please reflect and verify while reasoning and put the answer in \\\\boxed{}.',\n",
    "    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n",
    "    'You are a helpful and reflective maths assistant, please reason step by step to put the answer in \\\\boxed{}.',\n",
    "    'You are the smartest maths expert in the world, please spike this question and put the answer in \\\\boxed{}.'\n",
    "]\n",
    "\n",
    "def make_next_prompt(text,round_idx):\n",
    "    default_prompt = thoughts[(round_idx+1)%len(thoughts)] #'No boxed answer found,please generate python code or put the answer within \\\\boxed{}.'\n",
    "    default_python_code = f\"print('{default_prompt}')\"\n",
    "    return default_python_code\n",
    "    \n",
    "def extract_python_code(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if matches:\n",
    "        ans = \"\\n\\n\".join(matches)\n",
    "        #print(f'Extracted python code: {ans}')\n",
    "        return ans\n",
    "    return \"\"\n",
    "    \n",
    "def extract_python_code_list(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    ans=[]\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    for m in matches:\n",
    "        ans.append(m)\n",
    "    return ans\n",
    "    \n",
    "def process_python_code(query):\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        new_rows.append(row)\n",
    "    ans = \"\\n\".join(new_rows)\n",
    "    print(f'Processed python code: {ans}')\n",
    "    return ans\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_boxed_texts(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return []\n",
    "    ans = []\n",
    "    for content in matches:\n",
    "        if content.isdigit():\n",
    "            num = int(content)\n",
    "        else:\n",
    "            nums = re.findall(r'\\d+', content)\n",
    "            if not nums:\n",
    "                continue\n",
    "            num = int(nums[-1])\n",
    "        ans.append(num % 1000)\n",
    "    return ans\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return -1\n",
    "    content = matches[0]\n",
    "    if content.isdigit():\n",
    "        num = int(content)\n",
    "    else:\n",
    "        nums = re.findall(r'\\d+', content)\n",
    "        if not nums:\n",
    "            return -1\n",
    "        num = int(nums[-1])\n",
    "    return num % 1000\n",
    "\n",
    "from collections import Counter\n",
    "def select_answer(answers):\n",
    "    valid_answers = []\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                if 1 < int(answer) < 999 and int(answer) % 100 > 0:\n",
    "                    valid_answers.append(int(answer))\n",
    "        except:\n",
    "            pass\n",
    "    if not valid_answers:\n",
    "        return 49\n",
    "    _, answer = sorted([(v,k) for k,v in Counter(valid_answers).items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c70fb5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.347060Z",
     "iopub.status.busy": "2024-11-30T00:05:32.346630Z",
     "iopub.status.idle": "2024-11-30T00:05:32.353264Z",
     "shell.execute_reply": "2024-11-30T00:05:32.352683Z"
    },
    "papermill": {
     "duration": 0.016639,
     "end_time": "2024-11-30T00:05:32.354702",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.338063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "class PythonREPL:\n",
    "    def __init__(self, timeout=8):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            stdout = result.stdout.strip()\n",
    "            stderr = result.stderr.strip()\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                return True, stdout\n",
    "            else:\n",
    "                # Process the error message to remove the temporary file path\n",
    "                # This makes the error message cleaner and more user-friendly\n",
    "                error_lines = stderr.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                # Include stdout in the error case\n",
    "                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n",
    "                return False, combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d778ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.371797Z",
     "iopub.status.busy": "2024-11-30T00:05:32.371254Z",
     "iopub.status.idle": "2024-11-30T00:05:32.398705Z",
     "shell.execute_reply": "2024-11-30T00:05:32.398128Z"
    },
    "papermill": {
     "duration": 0.037395,
     "end_time": "2024-11-30T00:05:32.400095",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.362700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_texts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for messages in [[{\"role\": \"user\", \"content\": \"hi\"}]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c8f9fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.417052Z",
     "iopub.status.busy": "2024-11-30T00:05:32.416809Z",
     "iopub.status.idle": "2024-11-30T00:05:32.421625Z",
     "shell.execute_reply": "2024-11-30T00:05:32.421063Z"
    },
    "papermill": {
     "duration": 0.014809,
     "end_time": "2024-11-30T00:05:32.423006",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.408197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,              # randomness of the sampling\n",
    "    min_p=0.01,\n",
    "    skip_special_tokens=True,     # Whether to skip special tokens in the output.\n",
    "    #max_tokens=1800,\n",
    "    max_tokens=32768,\n",
    "    #stop=[\"```output\"],\n",
    ")\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    \n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "        print(messages[-1])\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ad56d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.440211Z",
     "iopub.status.busy": "2024-11-30T00:05:32.439667Z",
     "iopub.status.idle": "2024-11-30T00:05:32.444220Z",
     "shell.execute_reply": "2024-11-30T00:05:32.443645Z"
    },
    "papermill": {
     "duration": 0.014574,
     "end_time": "2024-11-30T00:05:32.445602",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.431028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_filter(list_of_messages,list_of_idx) -> tuple[list[list[dict]], list[str]]:\n",
    "    global answer_contributions\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    list_of_idx_to_keep = []\n",
    "    for idx,messages in zip(list_of_idx,list_of_messages):\n",
    "        answers = extract_boxed_texts(messages[-1]['content'])\n",
    "        if answers:\n",
    "            extracted_answers.extend(answers)\n",
    "            for answer in answers:\n",
    "                answer_contributions[answer].append(idx)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "            list_of_idx_to_keep.append(idx)\n",
    "    return list_of_messages_to_keep, extracted_answers, list_of_idx_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b0fd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.462665Z",
     "iopub.status.busy": "2024-11-30T00:05:32.462433Z",
     "iopub.status.idle": "2024-11-30T00:05:32.470954Z",
     "shell.execute_reply": "2024-11-30T00:05:32.470348Z"
    },
    "papermill": {
     "duration": 0.018759,
     "end_time": "2024-11-30T00:05:32.472354",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.453595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_execute(list_of_messages,round_idx) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'],round_idx)\n",
    "        python_code = process_python_code(python_code)\n",
    "        try:\n",
    "            success, output = PythonREPL()(python_code)\n",
    "        except Exception as e:\n",
    "            output = str(e)\n",
    "        messages.append({'role': 'user', 'content': output})\n",
    "        print(messages[-1])\n",
    "    return list_of_messages\n",
    "\n",
    "def batch_message_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
    "    #提取python代码，执行并获取答案，直接返回答案，不需要返回新的message\n",
    "    ans = []\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        try:\n",
    "            success, output = PythonREPL()(python_code)\n",
    "            if success:\n",
    "                patten = r'(\\d+)'\n",
    "                matches = re.findall(patten, output)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        ans.append(int(match)%1000)\n",
    "                        ans.append(int(match)%1000) #代码权重高于自然语言，所以添加两次 \n",
    "        except Exception as e:\n",
    "            output = str(e)\n",
    "        print(f'python code output: {output}')\n",
    "    return ans\n",
    "\n",
    "def batch_message_list_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
    "    #提取python代码，执行并获取答案，直接返回答案，不需要返回新的message\n",
    "    ans = []\n",
    "    for messages in list_of_messages:\n",
    "        python_code_list = extract_python_code_list(messages[-1]['content'])\n",
    "        for python_code in python_code_list:\n",
    "            python_code = process_python_code(python_code)\n",
    "            try:\n",
    "                success, output = PythonREPL()(python_code)\n",
    "                if success:\n",
    "                    patten = r'(\\d+)'\n",
    "                    matches = re.findall(patten, output)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            ans.append(int(match)%1000)\n",
    "                            ans.append(int(match)%1000) #代码权重高于自然语言，所以添加两次 \n",
    "            except Exception as e:\n",
    "                output = str(e)\n",
    "            print(f'python code output: {output}')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952e4ada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.489664Z",
     "iopub.status.busy": "2024-11-30T00:05:32.489145Z",
     "iopub.status.idle": "2024-11-30T00:05:32.492223Z",
     "shell.execute_reply": "2024-11-30T00:05:32.491650Z"
    },
    "papermill": {
     "duration": 0.013106,
     "end_time": "2024-11-30T00:05:32.493602",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.480496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b3004c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.511033Z",
     "iopub.status.busy": "2024-11-30T00:05:32.510509Z",
     "iopub.status.idle": "2024-11-30T00:05:32.514569Z",
     "shell.execute_reply": "2024-11-30T00:05:32.514019Z"
    },
    "papermill": {
     "duration": 0.014393,
     "end_time": "2024-11-30T00:05:32.516101",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.501708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_correct_answer(question):\n",
    "    if 'Three airline' in question: return 79\n",
    "    if 'Fred and George' in question: return 250\n",
    "    if 'Triangle $ABC$' in question: return 180\n",
    "    if 'Find the three' in question: return 143\n",
    "    if 'We call a' in question: return 3\n",
    "    if 'Let $ABC$ be' in question: return 751\n",
    "    if 'For a positive' in question: return 891\n",
    "    if 'For positive integers' in question: return 810\n",
    "    if 'The Fibonacci numbers' in question: return 201\n",
    "    if 'Alice writes all' in question: return 902\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36442d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.533362Z",
     "iopub.status.busy": "2024-11-30T00:05:32.532935Z",
     "iopub.status.idle": "2024-11-30T00:05:32.540474Z",
     "shell.execute_reply": "2024-11-30T00:05:32.539918Z"
    },
    "papermill": {
     "duration": 0.017666,
     "end_time": "2024-11-30T00:05:32.541878",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.524212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "g_score = 0\n",
    "g_count = 0\n",
    "prompt_score = Counter()\n",
    "answer_contributions = defaultdict(list)\n",
    "def predict_for_question(question: str) -> int:\n",
    "    global g_score\n",
    "    global g_count\n",
    "    global prompt_score\n",
    "    global answer_contributions\n",
    "    question += \"\\nIf the final answer is a number larger than 1000, take modulo 1000. \"\n",
    "    if time.time() > cutoff_time or not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        return 210\n",
    "    print(question)\n",
    "    \n",
    "    list_of_messages = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": thoughts[k]},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ] for k in range(5)\n",
    "    ]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    list_of_idx = list(range(len(list_of_messages)))\n",
    "    max_round = 1\n",
    "    for round_idx in range(max_round):\n",
    "        print(f\"round {round_idx+1}\")\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        #extracted_python_answer = batch_message_execute_and_get_answer(list_of_messages,round_idx)\n",
    "        extracted_python_answer = batch_message_list_execute_and_get_answer(list_of_messages,round_idx)\n",
    "        list_of_messages, extracted_answers, list_of_idx  = batch_message_filter(list_of_messages, list_of_idx)\n",
    "        all_extracted_answers.extend(extracted_python_answer)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        print(\"extracted boxed answers:\",extracted_answers)\n",
    "        print(\"extracted python answers:\",extracted_python_answer)\n",
    "        print(\"all extracted answers:\",all_extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "        #list_of_messages = batch_message_execute(list_of_messages,round_idx)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(\"answer:\",answer)\n",
    "    correct_answer = get_correct_answer(question)\n",
    "    print(\"correct answer:\",correct_answer)\n",
    "    g_count += 1\n",
    "    if str(answer) == str(correct_answer):\n",
    "        g_score += 1\n",
    "    # #计算贡献，将答案对应的prompt的分数都加1\n",
    "    # for prompt_idx in answer_contributions[correct_answer]:\n",
    "    #     prompt_score[prompt_idx%len(system_prompt)] += 1\n",
    "    # print(f'prompt score: {prompt_score}')\n",
    "    # if prompt_score:\n",
    "    #     best_prompt_idx = prompt_score.most_common(1)[0][0]\n",
    "    #     print(f'best prompt idx: {best_prompt_idx}')\n",
    "    #     print(f'best prompt: {system_prompt[best_prompt_idx%len(system_prompt)]}')\n",
    "    print(f\"score: {g_score}/{g_count}\")\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb14ab04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.559273Z",
     "iopub.status.busy": "2024-11-30T00:05:32.558727Z",
     "iopub.status.idle": "2024-11-30T00:05:32.562827Z",
     "shell.execute_reply": "2024-11-30T00:05:32.562226Z"
    },
    "papermill": {
     "duration": 0.014126,
     "end_time": "2024-11-30T00:05:32.564171",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.550045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ec9df1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.581561Z",
     "iopub.status.busy": "2024-11-30T00:05:32.581063Z",
     "iopub.status.idle": "2024-11-30T00:05:32.634749Z",
     "shell.execute_reply": "2024-11-30T00:05:32.634169Z"
    },
    "papermill": {
     "duration": 0.063912,
     "end_time": "2024-11-30T00:05:32.636233",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.572321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb98cb2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T00:05:32.653794Z",
     "iopub.status.busy": "2024-11-30T00:05:32.653267Z",
     "iopub.status.idle": "2024-11-30T00:05:33.105497Z",
     "shell.execute_reply": "2024-11-30T00:05:33.104770Z"
    },
    "papermill": {
     "duration": 0.462708,
     "end_time": "2024-11-30T00:05:33.107165",
     "exception": false,
     "start_time": "2024-11-30T00:05:32.644457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "057f8a\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1acac0\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "88c219\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "a1d40b\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "192e23\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "bbd91e\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1fce4b\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "71beb6\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "349493\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "480182\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 176602,
     "modelInstanceId": 154124,
     "sourceId": 180858,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 172131,
     "modelInstanceId": 154560,
     "sourceId": 181353,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 277.102782,
   "end_time": "2024-11-30T00:05:36.835081",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-30T00:00:59.732299",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "262d48bb13bb4b50ad56dbf3b1844d2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "387a9c1cd6194dc98cd7c012bf5c80b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d8274daef9624fba94df2d4133594699",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3d1e793e03ab4742bce17c3694058f25",
       "value": 5.0
      }
     },
     "3d1e793e03ab4742bce17c3694058f25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3d25be5b43dd464b96842f4e04227588": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8e3e5d70f0de4264bc46180d2c9a5ca4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e08962f0593c4726b627d325f1e015e0",
        "IPY_MODEL_387a9c1cd6194dc98cd7c012bf5c80b0",
        "IPY_MODEL_bdc086a604b249d19d31a8fc62cf23d8"
       ],
       "layout": "IPY_MODEL_ec11377f829c4f48b933d272f2b0b9e3"
      }
     },
     "a49d9266035d4212a6b42304b442580f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bdc086a604b249d19d31a8fc62cf23d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a49d9266035d4212a6b42304b442580f",
       "placeholder": "​",
       "style": "IPY_MODEL_3d25be5b43dd464b96842f4e04227588",
       "value": "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:00&lt;00:00, 26.26s/it]\n"
      }
     },
     "cde1cf17c2714e41b50b70ed1a648edb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8274daef9624fba94df2d4133594699": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e08962f0593c4726b627d325f1e015e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cde1cf17c2714e41b50b70ed1a648edb",
       "placeholder": "​",
       "style": "IPY_MODEL_262d48bb13bb4b50ad56dbf3b1844d2b",
       "value": ""
      }
     },
     "ec11377f829c4f48b933d272f2b0b9e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
