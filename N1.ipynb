{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759eafdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T02:51:38.263408Z",
     "iopub.status.busy": "2024-10-25T02:51:38.262446Z",
     "iopub.status.idle": "2024-10-25T02:51:38.269929Z",
     "shell.execute_reply": "2024-10-25T02:51:38.26859Z",
     "shell.execute_reply.started": "2024-10-25T02:51:38.263362Z"
    },
    "papermill": {
     "duration": 0.005215,
     "end_time": "2024-12-21T01:32:49.611891",
     "exception": false,
     "start_time": "2024-12-21T01:32:49.606676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "- https://www.kaggle.com/code/richolson/ai-math-olympiad-qwen2-5-72b for showing how to submit\n",
    "- https://www.kaggle.com/code/abdullahmeda/load-72b-awq-model-using-vllm-on-l4-x4\n",
    "- https://www.kaggle.com/code/huikang/qwen2-5-math-1-5b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9524704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:32:49.622235Z",
     "iopub.status.busy": "2024-12-21T01:32:49.621653Z",
     "iopub.status.idle": "2024-12-21T01:33:17.382311Z",
     "shell.execute_reply": "2024-12-21T01:33:17.381546Z"
    },
    "papermill": {
     "duration": 27.767776,
     "end_time": "2024-12-21T01:33:17.384281",
     "exception": false,
     "start_time": "2024-12-21T01:32:49.616505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42) #42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f706723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:33:17.395018Z",
     "iopub.status.busy": "2024-12-21T01:33:17.394532Z",
     "iopub.status.idle": "2024-12-21T01:33:18.180850Z",
     "shell.execute_reply": "2024-12-21T01:33:18.180141Z"
    },
    "papermill": {
     "duration": 0.793542,
     "end_time": "2024-12-21T01:33:18.182804",
     "exception": false,
     "start_time": "2024-12-21T01:33:17.389262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "cutoff_time = time.time() + (4 * 60 + 45) * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b755fb6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-21T01:33:18.193241Z",
     "iopub.status.busy": "2024-12-21T01:33:18.192981Z",
     "iopub.status.idle": "2024-12-21T01:37:15.567841Z",
     "shell.execute_reply": "2024-12-21T01:37:15.566952Z"
    },
    "papermill": {
     "duration": 237.382264,
     "end_time": "2024-12-21T01:37:15.569873",
     "exception": false,
     "start_time": "2024-12-21T01:33:18.187609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 01:33:37,001\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-21 01:34:02 awq_marlin.py:97] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 12-21 01:34:02 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 12-21 01:34:02 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1', speculative_config=None, tokenizer='/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 12-21 01:34:02 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-21 01:34:03 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 12-21 01:34:04 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-21 01:34:04 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-21 01:34:04 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-21 01:34:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 12-21 01:34:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 12-21 01:34:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 12-21 01:34:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 12-21 01:34:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 12-21 01:34:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 12-21 01:34:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 12-21 01:34:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 12-21 01:34:06 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m WARNING 12-21 01:34:06 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-21 01:34:06 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-21 01:34:06 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-21 01:34:06 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x787580716110>, local_subscribe_port=38805, remote_subscribe_port=None)\n",
      "INFO 12-21 01:34:06 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 12-21 01:34:06 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n",
      "INFO 12-21 01:34:06 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n",
      "INFO 12-21 01:34:06 model_runner.py:1056] Starting to load model /kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9d7053a65741d58babc892393c2297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 12-21 01:36:11 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "INFO 12-21 01:36:12 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 12-21 01:36:12 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 12-21 01:36:12 model_runner.py:1067] Loading model weights took 4.5459 GB\n",
      "INFO 12-21 01:36:36 distributed_gpu_executor.py:57] # GPU blocks: 13920, # CPU blocks: 4096\n",
      "INFO 12-21 01:36:36 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 6.80x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 12-21 01:36:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-21 01:36:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-21 01:36:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 12-21 01:36:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-21 01:36:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-21 01:36:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-21 01:36:39 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-21 01:36:39 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 12-21 01:37:15 model_runner.py:1523] Graph capturing finished in 36 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 12-21 01:37:15 model_runner.py:1523] Graph capturing finished in 36 secs.\n",
      "INFO 12-21 01:37:15 model_runner.py:1523] Graph capturing finished in 36 secs.\n",
      "INFO 12-21 01:37:15 model_runner.py:1523] Graph capturing finished in 36 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#llm_model_pth = '/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1'\n",
    "#llm_model_pth = '/kaggle/input/qwq-32b-preview/transformers/default/1'\n",
    "llm_model_pth = '/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1'\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    #dtype=\"half\",                # The data type for the model weights and activations\n",
    "    #max_num_seqs=128,              # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=32768,#4096*10,          # Model context length\n",
    "    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,      # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.96, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbc4d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.588111Z",
     "iopub.status.busy": "2024-12-21T01:37:15.587376Z",
     "iopub.status.idle": "2024-12-21T01:37:15.591179Z",
     "shell.execute_reply": "2024-12-21T01:37:15.590541Z"
    },
    "papermill": {
     "duration": 0.014304,
     "end_time": "2024-12-21T01:37:15.592592",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.578288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebf1510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.609808Z",
     "iopub.status.busy": "2024-12-21T01:37:15.609347Z",
     "iopub.status.idle": "2024-12-21T01:37:15.621069Z",
     "shell.execute_reply": "2024-12-21T01:37:15.620465Z"
    },
    "papermill": {
     "duration": 0.021838,
     "end_time": "2024-12-21T01:37:15.622483",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.600645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "thoughts = [\n",
    "    #'''你是一个乐于助人、人畜无害的数学天才。你应该使用 python 脚本并一步一步地思考，并把最后答案放在\\\\boxed{}中。''',\n",
    "    'Please use chained reasoning to put the answer in \\\\boxed{}.',\n",
    "    'Please reflect and verify while reasoning and put the answer in \\\\boxed{}.',\n",
    "    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n",
    "    'You are a helpful and reflective maths assistant, please reason step by step to put the answer in \\\\boxed{}.',\n",
    "    'You are the smartest maths expert in the world, please spike this question and put the answer in \\\\boxed{}.'\n",
    "]\n",
    "\n",
    "def make_next_prompt(text,round_idx):\n",
    "    default_prompt = thoughts[(round_idx+1)%len(thoughts)] #'No boxed answer found,please generate python code or put the answer within \\\\boxed{}.'\n",
    "    default_python_code = f\"print('{default_prompt}')\"\n",
    "    return default_python_code\n",
    "    \n",
    "def extract_python_code(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if matches:\n",
    "        ans = \"\\n\\n\".join(matches)\n",
    "        #print(f'Extracted python code: {ans}')\n",
    "        return ans\n",
    "    return \"\"\n",
    "    \n",
    "def extract_python_code_list(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    ans=[]\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    for m in matches:\n",
    "        ans.append(m)\n",
    "    return ans\n",
    "    \n",
    "def process_python_code(query):\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        new_rows.append(row)\n",
    "    ans = \"\\n\".join(new_rows)\n",
    "    print(f'Processed python code: {ans}')\n",
    "    return ans\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_boxed_texts(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return []\n",
    "    ans = []\n",
    "    for content in matches:\n",
    "        if content.isdigit():\n",
    "            num = int(content)\n",
    "        else:\n",
    "            nums = re.findall(r'\\d+', content)\n",
    "            if not nums:\n",
    "                continue\n",
    "            num = int(nums[-1])\n",
    "        ans.append(num % 1000)\n",
    "    return ans\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return -1\n",
    "    content = matches[0]\n",
    "    if content.isdigit():\n",
    "        num = int(content)\n",
    "    else:\n",
    "        nums = re.findall(r'\\d+', content)\n",
    "        if not nums:\n",
    "            return -1\n",
    "        num = int(nums[-1])\n",
    "    return num % 1000\n",
    "\n",
    "from collections import Counter\n",
    "def select_answer(answers):\n",
    "    valid_answers = []\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                if 1 < int(answer) < 999 and int(answer) % 100 > 0:\n",
    "                    valid_answers.append(int(answer))\n",
    "        except:\n",
    "            pass\n",
    "    if not valid_answers:\n",
    "        return 49\n",
    "    _, answer = sorted([(v,k) for k,v in Counter(valid_answers).items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126e8790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.639683Z",
     "iopub.status.busy": "2024-12-21T01:37:15.639167Z",
     "iopub.status.idle": "2024-12-21T01:37:15.645940Z",
     "shell.execute_reply": "2024-12-21T01:37:15.645305Z"
    },
    "papermill": {
     "duration": 0.016849,
     "end_time": "2024-12-21T01:37:15.647372",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.630523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "class PythonREPL:\n",
    "    def __init__(self, timeout=8):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            stdout = result.stdout.strip()\n",
    "            stderr = result.stderr.strip()\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                return True, stdout\n",
    "            else:\n",
    "                # Process the error message to remove the temporary file path\n",
    "                # This makes the error message cleaner and more user-friendly\n",
    "                error_lines = stderr.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                # Include stdout in the error case\n",
    "                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n",
    "                return False, combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc20c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.664538Z",
     "iopub.status.busy": "2024-12-21T01:37:15.663999Z",
     "iopub.status.idle": "2024-12-21T01:37:15.695373Z",
     "shell.execute_reply": "2024-12-21T01:37:15.694758Z"
    },
    "papermill": {
     "duration": 0.041445,
     "end_time": "2024-12-21T01:37:15.696815",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.655370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_texts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for messages in [[{\"role\": \"user\", \"content\": \"hi\"}]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cbe1b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.714010Z",
     "iopub.status.busy": "2024-12-21T01:37:15.713567Z",
     "iopub.status.idle": "2024-12-21T01:37:15.718564Z",
     "shell.execute_reply": "2024-12-21T01:37:15.717962Z"
    },
    "papermill": {
     "duration": 0.014971,
     "end_time": "2024-12-21T01:37:15.719892",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.704921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,              # randomness of the sampling\n",
    "    min_p=0.01,\n",
    "    skip_special_tokens=True,     # Whether to skip special tokens in the output.\n",
    "    #max_tokens=1800,\n",
    "    max_tokens=32768,\n",
    "    #stop=[\"```output\"],\n",
    ")\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    \n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "        print(messages[-1])\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658fdc33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.737136Z",
     "iopub.status.busy": "2024-12-21T01:37:15.736616Z",
     "iopub.status.idle": "2024-12-21T01:37:15.741269Z",
     "shell.execute_reply": "2024-12-21T01:37:15.740642Z"
    },
    "papermill": {
     "duration": 0.014721,
     "end_time": "2024-12-21T01:37:15.742672",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.727951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_filter(list_of_messages,list_of_idx) -> tuple[list[list[dict]], list[str]]:\n",
    "    global answer_contributions\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    list_of_idx_to_keep = []\n",
    "    for idx,messages in zip(list_of_idx,list_of_messages):\n",
    "        answers = extract_boxed_texts(messages[-1]['content'])\n",
    "        if answers:\n",
    "            extracted_answers.extend(answers)\n",
    "            for answer in answers:\n",
    "                answer_contributions[answer].append(idx)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "            list_of_idx_to_keep.append(idx)\n",
    "    return list_of_messages_to_keep, extracted_answers, list_of_idx_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1548f4fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.760219Z",
     "iopub.status.busy": "2024-12-21T01:37:15.759611Z",
     "iopub.status.idle": "2024-12-21T01:37:15.768559Z",
     "shell.execute_reply": "2024-12-21T01:37:15.767956Z"
    },
    "papermill": {
     "duration": 0.019288,
     "end_time": "2024-12-21T01:37:15.770014",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.750726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_execute(list_of_messages,round_idx) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'],round_idx)\n",
    "        python_code = process_python_code(python_code)\n",
    "        try:\n",
    "            success, output = PythonREPL()(python_code)\n",
    "        except Exception as e:\n",
    "            output = str(e)\n",
    "        messages.append({'role': 'user', 'content': output})\n",
    "        print(messages[-1])\n",
    "    return list_of_messages\n",
    "\n",
    "def batch_message_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
    "    #提取python代码，执行并获取答案，直接返回答案，不需要返回新的message\n",
    "    ans = []\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        try:\n",
    "            success, output = PythonREPL()(python_code)\n",
    "            if success:\n",
    "                patten = r'(\\d+)'\n",
    "                matches = re.findall(patten, output)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        ans.append(int(match)%1000)\n",
    "                        ans.append(int(match)%1000) #代码权重高于自然语言，所以添加两次 \n",
    "        except Exception as e:\n",
    "            output = str(e)\n",
    "        print(f'python code output: {output}')\n",
    "    return ans\n",
    "\n",
    "def batch_message_list_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
    "    #提取python代码，执行并获取答案，直接返回答案，不需要返回新的message\n",
    "    ans = []\n",
    "    for messages in list_of_messages:\n",
    "        python_code_list = extract_python_code_list(messages[-1]['content'])\n",
    "        for python_code in python_code_list:\n",
    "            python_code = process_python_code(python_code)\n",
    "            try:\n",
    "                success, output = PythonREPL()(python_code)\n",
    "                if success:\n",
    "                    patten = r'(\\d+)'\n",
    "                    matches = re.findall(patten, output)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            ans.append(int(match)%1000)\n",
    "                            ans.append(int(match)%1000) #代码权重高于自然语言，所以添加两次 \n",
    "            except Exception as e:\n",
    "                output = str(e)\n",
    "            print(f'python code output: {output}')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf0dcae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.787241Z",
     "iopub.status.busy": "2024-12-21T01:37:15.786812Z",
     "iopub.status.idle": "2024-12-21T01:37:15.789950Z",
     "shell.execute_reply": "2024-12-21T01:37:15.789338Z"
    },
    "papermill": {
     "duration": 0.013264,
     "end_time": "2024-12-21T01:37:15.791391",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.778127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97d842e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.808751Z",
     "iopub.status.busy": "2024-12-21T01:37:15.808344Z",
     "iopub.status.idle": "2024-12-21T01:37:15.812470Z",
     "shell.execute_reply": "2024-12-21T01:37:15.811892Z"
    },
    "papermill": {
     "duration": 0.014362,
     "end_time": "2024-12-21T01:37:15.813814",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.799452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_correct_answer(question):\n",
    "    if 'Three airline' in question: return 79\n",
    "    if 'Fred and George' in question: return 250\n",
    "    if 'Triangle $ABC$' in question: return 180\n",
    "    if 'Find the three' in question: return 143\n",
    "    if 'We call a' in question: return 3\n",
    "    if 'Let $ABC$ be' in question: return 751\n",
    "    if 'For a positive' in question: return 891\n",
    "    if 'For positive integers' in question: return 810\n",
    "    if 'The Fibonacci numbers' in question: return 201\n",
    "    if 'Alice writes all' in question: return 902\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86dd1ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.831139Z",
     "iopub.status.busy": "2024-12-21T01:37:15.830710Z",
     "iopub.status.idle": "2024-12-21T01:37:15.838355Z",
     "shell.execute_reply": "2024-12-21T01:37:15.837748Z"
    },
    "papermill": {
     "duration": 0.01781,
     "end_time": "2024-12-21T01:37:15.839755",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.821945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "g_score = 0\n",
    "g_count = 0\n",
    "prompt_score = Counter()\n",
    "answer_contributions = defaultdict(list)\n",
    "def predict_for_question(question: str) -> int:\n",
    "    global g_score\n",
    "    global g_count\n",
    "    global prompt_score\n",
    "    global answer_contributions\n",
    "    question += \"\\nIf the final answer is a number larger than 1000, take modulo 1000. \"\n",
    "    if time.time() > cutoff_time or not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        return 210\n",
    "    print(question)\n",
    "    \n",
    "    list_of_messages = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": thoughts[k]},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ] for k in range(5)\n",
    "    ]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    list_of_idx = list(range(len(list_of_messages)))\n",
    "    max_round = 1\n",
    "    for round_idx in range(max_round):\n",
    "        print(f\"round {round_idx+1}\")\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        #extracted_python_answer = batch_message_execute_and_get_answer(list_of_messages,round_idx)\n",
    "        extracted_python_answer = batch_message_list_execute_and_get_answer(list_of_messages,round_idx)\n",
    "        list_of_messages, extracted_answers, list_of_idx  = batch_message_filter(list_of_messages, list_of_idx)\n",
    "        all_extracted_answers.extend(extracted_python_answer)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        print(\"extracted boxed answers:\",extracted_answers)\n",
    "        print(\"extracted python answers:\",extracted_python_answer)\n",
    "        print(\"all extracted answers:\",all_extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "        #list_of_messages = batch_message_execute(list_of_messages,round_idx)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(\"answer:\",answer)\n",
    "    correct_answer = get_correct_answer(question)\n",
    "    print(\"correct answer:\",correct_answer)\n",
    "    g_count += 1\n",
    "    if str(answer) == str(correct_answer):\n",
    "        g_score += 1\n",
    "    # #计算贡献，将答案对应的prompt的分数都加1\n",
    "    # for prompt_idx in answer_contributions[correct_answer]:\n",
    "    #     prompt_score[prompt_idx%len(system_prompt)] += 1\n",
    "    # print(f'prompt score: {prompt_score}')\n",
    "    # if prompt_score:\n",
    "    #     best_prompt_idx = prompt_score.most_common(1)[0][0]\n",
    "    #     print(f'best prompt idx: {best_prompt_idx}')\n",
    "    #     print(f'best prompt: {system_prompt[best_prompt_idx%len(system_prompt)]}')\n",
    "    print(f\"score: {g_score}/{g_count}\")\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dedb69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.857168Z",
     "iopub.status.busy": "2024-12-21T01:37:15.856714Z",
     "iopub.status.idle": "2024-12-21T01:37:15.860838Z",
     "shell.execute_reply": "2024-12-21T01:37:15.860243Z"
    },
    "papermill": {
     "duration": 0.014276,
     "end_time": "2024-12-21T01:37:15.862276",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.848000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aa6e178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.879676Z",
     "iopub.status.busy": "2024-12-21T01:37:15.879270Z",
     "iopub.status.idle": "2024-12-21T01:37:15.927626Z",
     "shell.execute_reply": "2024-12-21T01:37:15.927014Z"
    },
    "papermill": {
     "duration": 0.058667,
     "end_time": "2024-12-21T01:37:15.929126",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.870459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cdfa73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T01:37:15.946737Z",
     "iopub.status.busy": "2024-12-21T01:37:15.946209Z",
     "iopub.status.idle": "2024-12-21T01:37:16.397238Z",
     "shell.execute_reply": "2024-12-21T01:37:16.396524Z"
    },
    "papermill": {
     "duration": 0.461505,
     "end_time": "2024-12-21T01:37:16.398866",
     "exception": false,
     "start_time": "2024-12-21T01:37:15.937361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "349493\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1fce4b\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "057f8a\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "480182\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "bbd91e\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "a1d40b\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "88c219\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "192e23\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "71beb6\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1acac0\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 176602,
     "modelInstanceId": 154124,
     "sourceId": 180858,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 172131,
     "modelInstanceId": 154560,
     "sourceId": 181353,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 273.949789,
   "end_time": "2024-12-21T01:37:20.128675",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-21T01:32:46.178886",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1ec874dc7e0a4993b44497c960f69cb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "35266bec3ea349649ca8d87f62b6435e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5ed2b9b855be45559d28bbd3144c4471": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_876a52ea37494f7288bde5df1ee9d938",
       "placeholder": "​",
       "style": "IPY_MODEL_1ec874dc7e0a4993b44497c960f69cb9",
       "value": "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:03&lt;00:00, 26.33s/it]\n"
      }
     },
     "6e9d7053a65741d58babc892393c2297": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cd685143dd2e4d40be05245e6e290e59",
        "IPY_MODEL_e93ea0f55992470b944df521bbad1b57",
        "IPY_MODEL_5ed2b9b855be45559d28bbd3144c4471"
       ],
       "layout": "IPY_MODEL_9c62c0d374b7461db7fb98069bfe2482"
      }
     },
     "876a52ea37494f7288bde5df1ee9d938": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a3469cee63e409b87cf155916e97e87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9c62c0d374b7461db7fb98069bfe2482": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd685143dd2e4d40be05245e6e290e59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4782040c2904ac4b29f3bef7a0bc0fd",
       "placeholder": "​",
       "style": "IPY_MODEL_8a3469cee63e409b87cf155916e97e87",
       "value": ""
      }
     },
     "dfe15d6b00d34b21bb3ed9dd140413b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e4782040c2904ac4b29f3bef7a0bc0fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e93ea0f55992470b944df521bbad1b57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_35266bec3ea349649ca8d87f62b6435e",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dfe15d6b00d34b21bb3ed9dd140413b5",
       "value": 5.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
